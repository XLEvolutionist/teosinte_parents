---
title: "Angsd analysis of first 20 Teosinte parents (Palmar Chico)"
author: "Simon Renny-Byfield"
date: "November 17, 2014"
output: html_vignette
---

This document is my record of the analysis performed on the first set of sequence data (20 individuals) avaialble for teosinte plants from PalMar Chico, Mexico. The document is modelled on an [earlier](https://github.com/XLEvolutionist/angsdworkflow) practice run on some dummy HapMap2 data. The new data are from the Palmar Chico population and have recently been sequences and UC Berkely. 

Vince Buffalo has maped and sorted the reads using his [paap.py](https://github.com/RILAB/paap) pipeline and the .bam files ready for input into [angsd](http://popgen.dk/wiki/index.php/ANGSD). On the other hand the mapping parameters make the .bam files unsuitable for use with [CNVer](http://compbio.cs.toronto.edu/CNVer/). This is because the mapping that is currently done has used paired end information and this currupts the CNVer analysis. Also the paired reads are not interleaved as required. The authors of CNVer strongly recommend using bowtie to map the data, and suggest some [parameters](http://compbio.cs.toronto.edu/cnver/README-0.8.1.txt) to use. This essentailly means that the reads will have to be mapped twice, once using BWA-MEM (for GLs, SFS, pi and Tajima's D) and again with [`bowtie`](http://bowtie-bio.sourceforge.net/index.shtml) for input to `CNVer`. As a first parse I am analysing only chromosome 10, in order that the full pipeline can be figured out. 

#Estimating the site frequency spectrum#

First estimate the site allele frequency likelihood. This requires several things listed below:

1. A file with listed, one per line, all the .bam files you want to analyse. You can grab the files you need by cdâ€™ing to the dir they are in and executing this code on the command line. 
```
ls -d $PWD/*.bam > file.list.txt
```
2. Choose which method you want to use with:
```
-doSaf [int 1-4]
```
There are four options listed in detail [here](http://popgen.dk/angsd/index.php/SFS_Estimation), in this case we want to estimate the **inbreeding co-efficient** of the sample, have this ready in a file and use the $-doSaf\space2$ option (See later for generating an inbreeding coefficient estiamtion).
3. Define your ancestral allele using the flag:
```{width=5}
-anc <path/to/referencegenome>
```
In my case we do not know the ancestral allele state, which means instead of derived allele SFS we need a minor allele SFS (a folded SFS). We can still provide an ancestral estimate using the reference genome (B73), but once folding is complete in becomes a minor allele SFS. We need to specify that we want a folded SFS with:
```
-fold 1
```
Or, alternatively we can estimate the ancestral state using *Tripsicum* reads mapped to the ref_v3 genome. We can supply a fasta file with tripsicum alleles placed on the ref_v3 resequence. The original file is stored here:
```
/group/jrigrp3/bottleneckProject/genomes/TRIP.fa
```
But I have a symbolic link in:
```
/home/sbyfield/teosinte_parents/genomes/TRIP.fa
```

4. Define the method for estimating Genotype Likelihoods:
```
-GL [int 1-4]
```
details of the different methods are provided [here](http://popgen.dk/angsd/index.php/Genotype_likelihoods). This will be important later as we need the GLs to estimate the **inbreeding coefficient**.

5. Define the number of processors to use with:
```
-P [int]
```

6. Define the outfile name using:
```
-out <path/to/outfile>
```
there are several output files and a suffix will be added to each file.

##Calculating the GL for each sample##

The .bam files are not indexed and so I wrote a quick script to get these indexed:
```
#!/bin/bash -l
#OUTDIR=/home/sbyfield/teosinte_parents/angsd_output
#SBATCH -D /home/sbyfield/teosinte_parents/angsd_output
#SBATCH -o /home/sbyfield/teosinte_parents/logs/out_log-%j.txt
#SBATCH -e /home/sbyfield/teosinte_parents/logs/err_log-%j.txt
#SBATCH --array=1-20
#SBATCH --mem-per-cpu=8000

##Simon Renny-Byfield, UC Davis, November 17 2014
##Usage: sbatch -p queue <file.sh> <first.list>

echo "Starting Job:"
date

index=0
while read line; do
   file1[index]="$line"
   let "index++"
done < $1 

#now index eaxh .bam file

samtools index ${file1[$SLURM_ARRAY_TASK_ID]}

echo "End Job: "
date
```


In order to calculate the inbreeding coefficient we need to know the genotype liklihoods of each sample. The genotype likelihood for each samples is calculated suing the below slurm script:
```
#!/bin/bash
#OUTDIR=/home/sbyfield/teosinte_parents/angsd_output
#SBATCH -D /home/sbyfield/teosinte_parents/angsd_output
#SBATCH -o /home/sbyfield/teosinte_parents/logs/out_log-%j.txt
#SBATCH -e /home/sbyfield/teosinte_parents/logs/err_log-%j.txt
#SBATCH -j geno

echo "Starting Job: "
date

COMMAND="angsd -bam /home/sbyfield/teosinte_parents/file.list.txt -doGlf 3 -GL 1 -out teo_parents20 -doMaf 2 -SNP_pval 1e-6 -doMajorMinor 1 -nThreads 16 -r 10 -minMapQ 30 -minQ 20"
echo $COMMAND

eval "$COMMAND"

echo "Ending Job: "
date
```
Note that in this case the $-GL\space1$ parameter means that the GLs are calculated using the SAMtools algorithm, detailed [here](https://www.broadinstitute.org/gatk/media/docs/Samtools.pdf). In addition only chromosome 10 is analysed in this case. You must add -doGlf 3 to output a sample based GL file in binary format. This is what `ngsF` needs to estimate $F$.

**********We are here so far******

##Estimating $F$ the inbreeding coefficient##

There is an [examples](https://github.com/fgvieira/ngsF/tree/master/examples) folder in the `ngsF` github repo which details how to use the program to generate $F$ for each sample. I will use this script based on the examples given:

```
#!/bin/bash
#OUTDIR=/home/sbyfield/teosinte_parents/angsd_output
#SBATCH -D /home/sbyfield/teosinte_parents/angsd_output
#SBATCH -o /home/sbyfield/teosinte_parents/logs/out_log-%j.txt
#SBATCH -e /home/sbyfield/teosinte_parents/logs/err_log-%j.txt
#SBATCH -J coefF

echo "Starting Job: "
date

N_SITES="$((`zcat teo_parents20.mafs.gz | wc -l`-1))"
echo $N_SITES

cmd1="ngsF -n_ind 20 -n_sites $N_SITES  -min_epsilon 0.001 -glf teo_parents20.glf -out teo_parents20.approx_indF -approx_EM -seed 12345 -init_values r -n_threads 8"
echo $cmd1
eval $cmd1

cmd2="ngsF -n_ind 20 -n_sites $N_SITES -min_epsilon 0.001 -glf teo_parents20.glf -out teo_parents20.indF -init_values teo_parents20.approx_indF.pars -n_threads 8"
echo $cmd2
eval $cmd2

echo "Job Done: "
date

```

This produces two output files, the first (in $cmd1) estiamtes $F$ and these estimates are parsed to a second run (in $cmd1) which then refines the estimates. Follwing this you can combine these estiamtes with the appropriate samples names using a unix command something like:
```
paste teo_parents20.approx_indF ../file.list.txt > teo_parents20.approx_named_indF.txt
```
where file.list.txt is the list of .bam files used in the analysis. Here is a histogram of $F$:
```{r eval=TRUE, echo=FALSE,fig.width=8,fig.height=4.5,dpi=300,out.width="680px",height="680px"}
F<-read.table("/Users/simonrenny-byfield/PalMarChico_data_info/Fcoeff_R.txt", header = FALSE)
hist(F[,1], breaks = 30, col="cornflowerblue", xlab="Inbreeding coefficient estimate", main = "Twenty Palmar Chico individuals")
mean(F[,1])
```

##Estimate the site allele frequency##

We can begin to estimate the site frequency spectrum. First we need the .saf file which can be generated with the below command:

```
#!/bin/bash
#OUTDIR=/home/sbyfield/teosinte_parents/angsd_output
#SBATCH -D /home/sbyfield/teosinte_parents/angsd_output
#SBATCH -o /home/sbyfield/teosinte_parents/logs/out_log-%j.txt
#SBATCH -e /home/sbyfield/teosinte_parents/logs/err_log-%j.txt
#SBATCH -J saf

echo "Starting Job: "
date

cmd="angsd -bam ../file.list.txt -doSaf 2 -out teoparents20 -anc ../genomes/TRIP.fa -ref ../genomes/Zea_mays.AGPv3.22.dna.genome.fa -GL 1 -P 12 -r 10 -indF teo_parents20.approx_indF -doMaf 1 -doMajorMinor 1 -minMapQ 30 -minQ 20"
echo $cmd
eval $cmd

echo "Job Done: "
date
```
I originally had `-doMaf 2`, but Tim has option `-doMaf 1`. After using `-doMaf 2` the SFS was:
```
-0.062885 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -2.797722
```
suggesting that the derived allele has a probability of being at frequency zero of `r exp(-0.062885)` and the derived allele at being at frequency 40 with probability `r exp(-2.797722)`. This seems terribly wrong. I have re-run the command with `-doMaf 1` option. This solved the problem and we now get this sort of output from `realSFS`.
```
-0.137286 -3.773777 -4.442083 -4.894909 -5.325430 -5.629502 -5.904913 -6.143009 -6.338800 -6.514279 -6.645978 -6.796172 -6.945955 -7.056051 -7.144333 -7.221669 -7.325251 -7.370765 -7.388119 -7.418594 -7.461358 -7.574166 -7.688638 -7.746183 -7.843669 -7.804058 -7.864351 -7.886032 -7.873087 -7.827869 -7.791916 -7.681828 -7.696670 -7.606883 -7.545436 -7.394282 -7.201138 -6.966641 -6.704915 -6.284160 -3.013013
```
##Formally calculate the SFS##

Next we need to use `realSFS` to construct the site frequency spectrum. Earlier in the dummy run on Hapmap2 data I generated a folded SFS, however in this case I am using an unfolded SFS as we have the ancestral genome of $Tripsicum$. Also, with a folded example you need to declare the number of individuals, whereas in this case it's the number of chromosomes (so for 10 individuals, you choose 20, and for 20 you choose 40). A command like this should work:
```
#!/bin/bash
#OUTDIR=/home/sbyfield/teosinte_parents/angsd_output
#SBATCH -D /home/sbyfield/teosinte_parents/angsd_output
#SBATCH -o /home/sbyfield/teosinte_parents/logs/out_log-%j.txt
#SBATCH -e /home/sbyfield/teosinte_parents/logs/err_log-%j.txt
#SBATCH -J sfs

echo "Starting Job: "
date

cmd="realSFS teoparents20.saf 40 -maxIter 100 -P 12 > teoparents20.sfs"
echo $cmd
eval $cmd

echo "Job Done: "
date
```

The SFS for the first 20 teosinte plants looks like this:
```{r,echo = FALSE,fig.width=8,fig.height=4.5,dpi=300,out.width="680px",height="680px"}
sfs<-exp(scan("/Users/simonrenny-byfield/test_angst/teoparents20.sfs")) 
barplot(sfs[-c(1,41)],col="cornflowerblue",names.arg=1:39, ylab="probability",xlab="allele freqquency", main = "Chr10")
```

#Estimating genome (or chromosome) wide thetas#

The next step is to caulculate the genome wide thetas with the following command:

```
#!/bin/bash
#OUTDIR=/home/sbyfield/teosinte_parents/angsd_output
#SBATCH -D /home/sbyfield/teosinte_parents/angsd_output
#SBATCH -o /home/sbyfield/teosinte_parents/logs/out_log-%j.txt
#SBATCH -e /home/sbyfield/teosinte_parents/logs/err_log-%j.txt
#SBATCH -J thetas

echo "Starting Job: "
date

cmd="angsd -bam ../file.list.txt -out teosinte20thetas.sfs -doThetas 1 -doSaf 1 -pest teoparents20.sfs -anc ../genomes/TRIP.fa -GL 2 -P 12 -r 10 -minMapQ 30 -minQ 20"
echo $cmd
eval $cmd

echo "Ending Job: "
date
```
Next we can make a BED file of the genome-wide theta values and calculate Tajima's D:
```
#!/bin/bash
#OUTDIR=/home/sbyfield/teosinte_parents/angsd_output
#SBATCH -D /home/sbyfield/teosinte_parents/angsd_output
#SBATCH -o /home/sbyfield/teosinte_parents/logs/out_log-%j.txt
#SBATCH -e /home/sbyfield/teosinte_parents/logs/err_log-%j.txt
#SBATCH -J thetas_bed

echo "Starting Job: "
date

#create a binary version of thete.thetas.gz 
thetaStat make_bed teosinte20thetas.sfs.thetas.gz
#calculate Tajimas D
thetaStat do_stat out.thetas.gz -nChr 20 -win 5000 -step 1000  -outnames teothetasWindow_chr10.gz

echo "Ending Job: "
date
```

I have tried a number of options for window size and slide. I'll list these below along with the associated file names. All output files are stores in the dir `/home/sbyfield/teosinte_parents/angsd_output` on `farm`.

* Window: 5000 Step: 1000 file: teothetasWindow5000bp_step_1000_chr10.gz.pestPG
* Window: 1000 Step: 500 file: teothetasWindow1000bp_step_500_chr10.gz.pestPG
* Window: 100 Step: 50 file: teothetasWindow100bp_step_50_chr10.gz.pestPG
* Window: 10 Step: 10 file: teothetasWindow10bp_step_10_chr10.gz.pestPG
* Window: 10 Step: 5 file: teothetasWindow10bp_step_5_chr10.gz.pestPG

#Estimate coverage per locus per individual#

The purpose of this aspect of the analysis is to generate estiamtes of coverage over specific base pairs. This will allow us to assess the effects on, SFS, pi and Tajima's D depending on the number of samples that are available.

```
#!/bin/bash
#OUTDIR=/home/sbyfield/teosinte_parents/angsd_output
#SBATCH -D /home/sbyfield/teosinte_parents/angsd_output
#SBATCH -o /home/sbyfield/teosinte_parents/logs/out_log-%j.txt
#SBATCH -e /home/sbyfield/teosinte_parents/logs/err_log-%j.txt
#SBATCH -J counts

echo "Starting Job: "
date

cmd="angsd -bam ../file.list.txt -doCounts 1 -minInd 0 -dumpCounts 2 -minQ 20 -P 12 -r 10 -out teoparents20_Chr10counts"
echo $cmd
eval $cmd

echo "Ending Job: "
date
```

The output of the this call (currently in teoparents20_Chr10counts.counts.gz) looks something like this:
```
ind0TotDepth  ind1TotDepth	ind2TotDepth	ind3TotDepth	ind4TotDepth	ind5TotDepth	ind6TotDepth	ind7TotDepth	ind8TotDepth	ind9TotDepth	ind10TotDepth	ind11TotDepth	ind12TotDepth	ind13TotDepth	ind14TotDepth	ind15TotDepth	ind16TotDepth	ind17TotDepth	ind18TotDepth	ind19TotDepth	
1	1	0	1	0	1	0	0	1	0	0	0	0	0	3	0	0	0	0	0	
1	1	2	3	0	3	0	1	1	0	0	0	2	1	4	0	0	0	0	0	
2	3	2	6	0	6	0	1	1	1	0	0	2	1	4	2	0	2	1	0	
3	3	2	6	2	7	0	1	1	1	0	1	4	1	5	4	0	3	1	0	
3	3	2	6	2	7	0	1	2	1	0	1	3	2	4	4	0	3	1	0	
11	3	2	6	2	9	0	1	1	1	2	1	4	2	5	4	0	3	1	0	
11	3	2	6	3	9	0	2	1	1	5	1	3	8	5	4	0	3	1	0	
12	3	2	6	3	11	0	2	2	4	8	1	8	11	7	5	0	5	2	0	
12	3	2	9	4	11	0	3	1	3	8	0	9	11	7	5	0	5	2	0
```
However, the positions of each read count are given in teoparents20_Chr10counts.pos.gz and looks like:
```
chr  pos	totDepth
10	19	8
10	20	19
10	21	34
10	22	45
10	23	45
10	24	58
10	25	68
10	26	92
10	27	95
```
There fore the rowsums of the first file (teoparents20_Chr10counts.counts.gz) should match the last column in the second file (teoparents20_Chr10counts.pos.gz).

#Plot some of the data#

plot chromsome wide Tajima's D for the forst 20 parents (using window size 5000, slide of 1000).
```{r,echo = FALSE,fig.width=8,fig.height=4.5,dpi=300,out.width="680px",height="680px"}
TJD<-read.table("/Users/simonrenny-byfield/test_angst/teothetasWindow5000bp_step_1000_chr10.gz.pestPG", header = T, sep = "\t")
plot(TJD[,3],TJD[,9], pch = 16, cex=.5, col = "cornflowerblue", xlab = "position", ylab = "Tajima's D")
```

Now what about TJD with coverage per window
```{r,echo = FALSE,fig.width=8,fig.height=4.5,dpi=300,out.width="680px",height="680px"}
library(ggplot2)
#load in the data
TJD<-read.table("/Users/simonrenny-byfield/test_angst/teothetasWindow_chr10.gz.pestPG", header = T, sep = "\t")
scatter.smooth(TJD[,14]/5000,TJD[,9], pch = 16, cex=.5, xlab="proportion oa bases covered", ylab="Tajima's D", col ="red")
```

And for pi.

```{r,echo = FALSE,fig.width=8,fig.height=4.5,dpi=300,out.width="680px",height="680px"}
TJD<-read.table("/Users/simonrenny-byfield/test_angst/teothetasWindow_chr10.gz.pestPG", header = T, sep = "\t")
scatter.smooth(TJD[,14]/5000,(TJD[,5]/TJD[,14]), pch = 16, cex=.5, xlab="proportion of bases covered", ylab = "pi", col = "red")
```

**Importantly some of the data has been moved. `~/teosinte_parents` has now moved to `/group/jrigrp4/` due to space issues in my home dir.**

#Theta, pi and Tajima's D for varying numbers of individuals#

Jeff pointed out that we should really take a look at how the minimum number of inifividuals affets estimates of Theta, pi and Taj D. I think this can be done by re-running the final steps of the analysis but with the `-minInd` flag set to the appropriate number (5,10,15,20 for example).

For example to run estiamtes of theta with a minimum of `mInd` individuals you would need..

```
#!/bin/bash
#OUTDIR=/group/jrigrp4/teosinte_parents/angsd_output
#SBATCH -D /group/jrigrp4/teosinte_parents/angsd_output
#SBATCH -o /group/jrigrp4/teosinte_parents/logs/out_log-%j.txt
#SBATCH -e /group/jrigrp4/teosinte_parents/logs/err_log-%j.txt
#SBATCH -J thetas

echo "Starting Job: "
date

for i in {5,10,15,20}
  do
    mInd=$i
    mkdir $i
    cmd="angsd -bam ../file.list.txt -out $i/teosinte20thetas.sfs -doThetas 1 -doSaf 1 -pest teoparents20.sfs -anc ../genomes/TRIP.fa -GL 2 -P 12 -r 10 -minMapQ 30 -minQ 20 -minInd $mInd"
    echo $cmd
    eval $cmd
 done
echo "Ending Job: "
date

```

next calculate Tajima's D for all these runs:

```
#!/bin/bash
#OUTDIR=/group/jrigrp4/teosinte_parents/angsd_output
#SBATCH -D /group/jrigrp4/teosinte_parents/angsd_output
#SBATCH -o /group/jrigrp4/teosinte_parents/logs/out_log-%j.txt
#SBATCH -e /group/jrigrp4/teosinte_parents/logs/err_log-%j.txt
#SBATCH -J thetas_bed

echo "Starting Job: "
date

for i in {5,10,15,20}
  do
    #create a binary version of thete.thetas.gz 
    cmd="thetaStat make_bed $i/teosinte20thetas$i.sfs.thetas.gz"
    echo $cmd
    #eval $cmd
    #calculate Tajimas D
    cmd="thetaStat do_stat $i/teosinte20thetas$i.sfs.thetas.gz -nChr 20 -win 5000 -step 1000  -outnames $i/teosinte20thetas$i_chr10.gz"
    echo $cmd
    eval $cmd
 done
 
echo "Ending Job: "
date
```

Now how does Tajima's D and pi vary when you consider different minimum numbers of individuals with data at a site. This will be done using the following script:
![pi](/Users/simonrenny-byfield/test_angst/pi_vs_minInd.png)
